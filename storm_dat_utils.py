# -*- coding: utf-8 -*-
"""
Created on Mon Nov 16 14:46:59 2020

@author: Dylan Manuel (xgk011)
"""
import numpy as np
import pandas as pd

DATA_PATH = 'Storm Data/' # Path to where all storm data files should be kept

def preprocessStormData(stormFrame, keepColumns=None, additionalColumns=None):
    '''
    Preprocesses raw storm data that was loaded from a .csv generated by the 
    Storm Events Database from the NOAA website to a DataFrame. By default, the 
    following columns are removed during preprocessing: EVENT_ID, EPISODE_ID, 
    CZ_TYPE, CZ_FIPS, WFO, SOURCE, BEGIN_RANGE, BEGIN_AZIMUTH, END_RANGE, END_AZIMUTH, 
    END_LOCATION, BEGIN_LAT, BEGIN_LON, END_LAT, END_LON, EVENT_NARRATIVE, EPISODE_NARRATIVE, 
    ABSOLUTE_ROWNUMBER, and CZ_TIMEZONE. In addition, all column headers are 
    named to appropriate headers and all strings are capitalized to avoid any 
    case-sensitive mismatches.

    Parameters
    ----------
    stormFrame : DataFrame
        A DataFrame containing the raw storm data loaded from a .csv.
    keepColumns : List, optional
        A list of strings representing any additional columns to keep that are 
        normally removed during the preprocessing. The default is None.
    additionalColumns : List, optional
        A list of string representing additional columns to remove that are 
        normally kept during the preprocessing. The default is None.

    Returns
    -------
    None.

    '''
    remove = ['EVENT_ID', 'EPISODE_ID', 'CZ_TYPE', 'CZ_FIPS', 'WFO', 'SOURCE', 
              'BEGIN_RANGE', 'BEGIN_AZIMUTH', 'END_RANGE', 'END_AZIMUTH', 
              'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 
              'EVENT_NARRATIVE', 'EPISODE_NARRATIVE', 'ABSOLUTE_ROWNUMBER', 
              'CZ_TIMEZONE'] # Columns to rmeove by default
    # Check if there are any columns to keep
    if not keepColumns is None:
        for column in keepColumns:
            remove.remove(column)
    # Check if there are any additional columns to remove
    if not additionalColumns is None:
        remove.extend(additionalColumns)
    # Replace all empty cells with NaN
    stormFrame.replace(' ', np.nan, inplace=True)
    # Drop columns that will not be used in any analyses
    stormFrame.drop(remove, axis=1, inplace=True)
    # Drop columns with all cells containing NaN
    stormFrame.dropna(axis=1, inplace=True, how='all')
    # Rename column headers
    stormFrame.rename(columns={'CZ_NAME_STR' : 'County', 
                               'BEGIN_LOCATION' : 'Location', 'BEGIN_DATE' : 'Date', 
                               'BEGIN_TIME' : 'Time', 'EVENT_TYPE' : 'Weather Event', 
                               'DEATHS_DIRECT' : 'Direct Deaths', 
                               'INJURIES_DIRECT' : 'Direct Injuries', 'DAMAGE_PROPERTY_NUM' : 'Property Damage', 
                               'DAMAGE_CROPS_NUM' : 'Damaged Crops', 'STATE_ABBR' : 'State', 
                               'INJURIES_INDIRECT' : 'Indirect Injuries', 'DEATHS_INDIRECT' : 'Indirect Deaths', 
                               'MAGNITUDE' : 'Magnitude', 'TOR_F_SCALE' : 'Tornado F-Scale', 
                               'MAGNITUDE_TYPE' : 'Magnitude Type', 'FLOOD_CAUSE' : 'Cause of Flood', 
                               'TOR_LENGTH' : 'Tornado Length', 'TOR_WIDTH' : 'Tornado Width'}, inplace=True)
    # Change all strings to uppercase
    for column in stormFrame:
        if stormFrame[column].dtype == 'object':
            stormFrame[column] = stormFrame[column].str.upper()

def loadMultiCSVs(*fileNames, removeDups=True):
    '''
    Loads multiple .csv files and concatenates the result into one DataFrame.

    Parameters
    ----------
    *fileNames : varying strings
        List of .csv file names to load.
    removeDups : bool, optional
        True if duplicate rows containing the exact same values on every column 
        should be removed. The default is True.

    Returns
    -------
    df : DataFrame
        DataFrame containing all the loaded .csv files.

    '''
    df = pd.DataFrame() # DataFrame containing all the csv files
    
    for fileName in fileNames:
        df = pd.concat([df, pd.read_csv(fileName)], axis=0)
    if removeDups:
        # Drop duplicate rows
        df.drop_duplicates(inplace=True)
    return df